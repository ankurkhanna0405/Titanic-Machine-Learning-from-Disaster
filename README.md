# Titanic-Machine-Learning-from-Disaster

Details on how to participate in this Kaggle competition are available here: https://www.kaggle.com/alexisbcook/titanic-tutorial

Initially I read the train and test csv data on Kaggle and combine them to understand the structure of the data as a whole and get better accurate results on missing data. Next I check the missing values and find that Cabin has the greatest number of missing values, 1014 values. Age has 263 missing values while Embarked and Fare have two and one missing values, respectively. For fare I impute the mean value of the column. For Embarked I impute the mode value that is ‘S’. for Age, I just checked the mean value as per Gender and there are quite close but not accurate enough, so I checked mean values as per the Pclass and I found there’s a huge difference. 
The passengers in the higher classes tend to be older. Rather than just imputing missing age values by the overall average for age, I used average age values of each class to impute missing age values based on Pclass. Then I changed features Survived, Pclass, Sex and Embarked to factor data type because they are categorical data. After cleaning data , I have split the data back to Train and Test data with limited features  eliminating Ticket and Cabin because it has many missing values and not a useful feature. Now I take the Train data and split it into training set and validation set to get accuracy results with 0.8 as the split ratio. I did not create any new features in the dataset.


I split the training set into the training set (80% of training data) and validation set (20% of training data) for the evaluation purposes of the fitted models. I try 4 models on this dataset – Logistic regression, Random forest , Naïve Bayes and SVM model. I have used confused matrix and misclassification accuracy to evaluate the model on the validation set.

For the Logistic regression model, when I run it on the training set, I took Survived as the dependent variable and all the other features as independent features. The features Pclass (‘2’ and ‘3’) Sex (Male) and SibSp significantly contribute to the model in predicting survival. I use the step() function iteratively to remove the insignificant features from the model. I have used confused matrix and misclassification accuracy to evaluate the model. I ran the model on the validation set and got accuracy of 82% and when on test data accuracy of 75.5%.  Similarly, I ran Random forest data with the same features and got accuracy of 83.7% on validation and 76.5% on test data. For naïve Bayes model I achieved accuracy of 80.3% with validation using 3 independent features – Sex, Age and Pclass. Accuracy on Test set was again 76.5%. For SVM model I used all features as independent variables and kernel as Radial basis and got an accuracy of 71.7% on validation set and 79% accuracy on test data set. Therefore, I chose the SVM model because it got the maximum accuracy on test among all.


After selecting SVM model, I changed the Kernel method with Linear method and Polynomial method. For linear I got accuracy of 77% on test data and for polynomial accuracy of 78.4% on test data. I tried changing the cost and gamma hyperparameters too but accuracy went down. So the best kernel is Nonlinear radial method with the maximum accuracy.



The model was selected according to the confusion matrix and SVM turned out to be the best model. For Logistic regression, Confusion matix consists of 9 false positives and 23 false negatives. Random forest, Confusion matix consists of 6 false positives and 24 false negatives. Naïve Bayes, Confusion matix consists of 13 false positives and 22 false negatives. And for Nonlinear SVM Radial, Confusion matix consists of 13 false positives and 4 false negatives. 

I found out that changing the numbers of independent features of the model changed the confusion matrix and misclassification error significantly. Secondly, every classifier has unique role in predicting data and used for particular type of data. Even though I went through many sample codes in the competition on Kaggle I found out that the best model running was random forest but still I got the best model as Nonlinear SVM with the maximum accuracy. I also learnt that changing the missing values with mean , mode or median changes the output result significantly and that is why we need to impute them after thorough learning of the dataset. Even though validation set error plays an important role in test set accuracy but there are other factors too. Models with high validation set accuracy may or may not be the best in test data accuracy. 
